# Text Summarizer - End-to-End ML Project

An end-to-end text summarization application using fine-tuned Pegasus transformer model. This project implements a complete machine learning pipeline from data ingestion to model deployment, with a user-friendly web interface for generating summaries.

## ğŸš€ Features

- **End-to-End ML Pipeline**: Complete workflow from data ingestion to model evaluation
- **Fine-tuned Pegasus Model**: Pre-trained on CNN/DailyMail and fine-tuned on SAMSum dataset
- **Web Interface**: FastAPI-based web application with a modern UI
- **RESTful API**: Easy-to-use API endpoints for text summarization
- **Model Training**: Automated training pipeline with configurable parameters
- **Model Evaluation**: Built-in evaluation metrics (ROUGE, BLEU scores)

## ğŸ› ï¸ Tech Stack

- **Framework**: FastAPI, Uvicorn
- **ML Library**: Transformers (Hugging Face), PyTorch
- **Data Processing**: Datasets, Pandas, NLTK
- **Evaluation**: ROUGE Score, SacreBLEU
- **Frontend**: Jinja2 Templates, HTML/CSS
- **Configuration**: PyYAML, Python-Box

## ğŸ“ Project Structure

```
text-summarizer/
â”œâ”€â”€ artifacts/                 # Generated artifacts (models, datasets, metrics)
â”‚   â”œâ”€â”€ data_ingestion/        # Raw and processed datasets
â”‚   â”œâ”€â”€ data_transformation/   # Tokenized datasets
â”‚   â”œâ”€â”€ model_trainer/         # Trained model checkpoints
â”‚   â””â”€â”€ model_evaluation/      # Evaluation metrics
â”œâ”€â”€ config/                    # Configuration files
â”‚   â””â”€â”€ config.yaml           # Main configuration
â”œâ”€â”€ research/                  # Jupyter notebooks for experimentation
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_data_validation.ipynb
â”‚   â”œâ”€â”€ 03_data_transformation.ipynb
â”‚   â”œâ”€â”€ 04_model_trainer.ipynb
â”‚   â””â”€â”€ 05_Model_evaluation.ipynb
â”œâ”€â”€ src/                       # Source code
â”‚   â””â”€â”€ text_summarizer/
â”‚       â”œâ”€â”€ components/        # Core components
â”‚       â”œâ”€â”€ config/           # Configuration management
â”‚       â”œâ”€â”€ pipeline/          # Training and prediction pipelines
â”‚       â””â”€â”€ utils/            # Utility functions
â”œâ”€â”€ static/                    # Static files (CSS, JS)
â”œâ”€â”€ templates/                 # HTML templates
â”œâ”€â”€ app.py                     # FastAPI application
â”œâ”€â”€ main.py                    # Training pipeline entry point
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ params.yaml               # Training hyperparameters
```

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- Conda (recommended) or pip
- Git

## ğŸ”§ Installation

### Step 1: Clone the Repository

```bash
git clone https://github.com/krishmaniyar/text_summarizer.git
cd text-summarizer
```

### Step 2: Create a Conda Environment

```bash
conda create -n summary python=3.8 -y
conda activate summary
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

This will install all required packages including:
- transformers
- torch
- fastapi
- uvicorn
- datasets
- rouge_score
- and more...

## ğŸ¯ Usage

### Running the Web Application

1. **Start the FastAPI server**:

```bash
python app.py
```

2. **Access the application**:

Open your browser and navigate to:
```
http://localhost:8080
```

3. **Generate Summaries**:

- Enter your text in the input field
- Click the "Summarize" button
- View the generated summary

### API Endpoints

#### 1. Home Page
- **URL**: `GET /`
- **Description**: Returns the main web interface

#### 2. Generate Summary
- **URL**: `POST /predict`
- **Description**: Generates a summary for the input text
- **Parameters**:
  - `text` (form-data): The text to summarize
- **Response**: HTML page with the summary

#### 3. Train Model
- **URL**: `GET /train`
- **Description**: Triggers the training pipeline
- **Response**: JSON with training status

### Example API Usage

Using `curl`:

```bash
curl -X POST "http://localhost:8080/predict" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "text=Your long text here that needs to be summarized..."
```

Using Python:

```python
import requests

url = "http://localhost:8080/predict"
data = {"text": "Your long text here..."}
response = requests.post(url, data=data)
print(response.text)
```

## ğŸ‹ï¸ Training the Model

### Running the Training Pipeline

To train the model from scratch, run:

```bash
python main.py
```

This will execute the complete pipeline:

1. **Data Ingestion**: Downloads and extracts the SAMSum dataset
2. **Data Validation**: Validates the dataset structure
3. **Data Transformation**: Tokenizes the data using Pegasus tokenizer
4. **Model Training**: Fine-tunes the Pegasus model on the dataset
5. **Model Evaluation**: Evaluates the model and generates metrics

### Training Configuration

Edit `params.yaml` to adjust training hyperparameters:

```yaml
TrainingArguments:
  num_train_epochs: 1
  warmup_steps: 500
  per_device_train_batch_size: 1
  weight_decay: 0.01
  logging_steps: 10
  evaluation_strategy: steps
  eval_steps: 500
  save_steps: 1e6
  gradient_accumulation_steps: 16
```

### Model Configuration

Edit `config/config.yaml` to modify:
- Data paths
- Model checkpoints
- Tokenizer settings
- Evaluation metrics

## ğŸ“Š Model Details

- **Base Model**: `google/pegasus-cnn_dailymail`
- **Fine-tuned Dataset**: SAMSum (conversation summarization)
- **Tokenizer**: Pegasus tokenizer
- **Generation Parameters**:
  - Length penalty: 0.8
  - Number of beams: 8
  - Max length: 128 tokens

## ğŸ”¬ Research Notebooks

The `research/` directory contains Jupyter notebooks for each stage of the pipeline:

- `01_data_ingestion.ipynb`: Data download and preprocessing
- `02_data_validation.ipynb`: Data quality checks
- `03_data_transformation.ipynb`: Tokenization and feature engineering
- `04_model_trainer.ipynb`: Model training experiments
- `05_Model_evaluation.ipynb`: Model evaluation and metrics

## ğŸ“ Configuration Files

### config.yaml
Main configuration file containing:
- Artifact root directories
- Data ingestion settings
- Model paths
- Evaluation settings

### params.yaml
Training hyperparameters and model arguments.

## ğŸ³ Docker Support

A `Dockerfile` is included for containerized deployment. Build and run:

```bash
docker build -t text-summarizer .
docker run -p 8080:8080 text-summarizer
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the terms specified in the LICENSE file.

## ğŸ‘¤ Author

**Krish Maniyar**

- Email: krishmaniyar27@gmail.com
- GitHub: [@krishmaniyar](https://github.com/krishmaniyar)

## ğŸ™ Acknowledgments

- Hugging Face for the Transformers library
- Google Research for the Pegasus model
- SAMSum dataset creators

## ğŸ“š Additional Resources

- [Pegasus Paper](https://arxiv.org/abs/1912.08777)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

**Note**: This project is based on the end-to-end ML project structure. Make sure to have sufficient computational resources (GPU recommended) for training the model.
